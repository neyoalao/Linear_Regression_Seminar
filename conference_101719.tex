\documentclass[conference]{IEEEtran}
%\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{url}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
    
    
\setcounter{secnumdepth}{5}
\setcounter{tocdepth}{5}

\makeatletter
\newcommand\subsubsubsection{\@startsection{paragraph}{4}{\z@}{-2.5ex\@plus -1ex \@minus -.25ex}{1.25ex \@plus .25ex}{\normalfont\normalsize\bfseries}}
\newcommand\subsubsubsubsection{\@startsection{subparagraph}{5}{\z@}{-2.5ex\@plus -1ex \@minus -.25ex}{1.25ex \@plus .25ex}{\normalfont\normalsize\bfseries}}
\makeatother

\newcommand{\ulsi}[1]{\!\overline{\,{#1}}} % underline short italic
\newcommand{\uls}[1]{\mskip.5\thinmuskip\underline{\mskip-.5\thinmuskip {#1} \mskip-.5\thinmuskip}\mskip.5\thinmuskip} % underline short
   
\begin{document}

\title{Linear Regression: Deep Learning}

\author{\IEEEauthorblockN{Olaniyi Bayonle Alao}
\IEEEauthorblockA{\textit{Summer Term, 2021} \\
\textit{Bachelor of Electronic Engineering} \\
\textit{Hochschule Hamm-Lippstadt}\\
Lippstadt, Germany \\
olaniyi-bayonle.alao@stud.hshl.de}
}

\maketitle

\begin{abstract}
Machine learning refers to is the ability of applications to get better at doing things without necessarily a change in the code base them is something that has really been helpful especially in this era of big data.
Even though machine learning is not a new concept, it is fast gaining recognization and changing lives thanks to the increase in the processing power of computers over the years to be able to process big data at a level that has never been experienced. This paper talks about linear regression algorithm in the context of machine learning. Linear regrression is a statictical term that uses a dependent and independent variable to make predictions. In these paper, the theorectical background of linear regression was explored. Predicting target values using the Scikit-Learn scientific library for machine learning  in python was made. The data used in this paper were gotten from the Scikit-Learn dataset repository library. The California housing dataset from this repository was used for training and prediction using linear regression. As a mention of how good a model as learnt, mean squared error and coefficient of determination were explained. As a conclusion, an evaulation of the trained model was discussed to understand how well it performed in predicting.
\end{abstract}
\begin{IEEEkeywords}
machine learning, linear regression, deep learning, sci-kit
\end{IEEEkeywords}
Unless otherwise stated, the main reference for facts, mathematical models and terminologies used in this research paper is gotten from \cite{massaron2016regression}

\section{Introduction}
Due to an explosive increase in the amount of data generated from different internet connected devices -Cyber-Physical-Systems (CPS)-, there has been an increase in the need to make sense of this "big data" to ensure the proper and productive functioning of businesses. By 2026, the data generation from global air fleet is projected to reach 98 billion gigabytes \cite{oliver_wyman}. Thanks to the recent advancements in the information processing capabilities of computers, making sense of this huge chunk of data is now possible using data analysis techniques and machine learning algorithms to clean and make prediction from this datasets. Machine learning is the ability of machine -software application- to learn and make predictions without being programmed to do so. They are able to learn and get better with an increase in the amount of data they are being fed through through their grounded mathemical foundations.

\quad Linear regression is a statistical test used to find out the relationship between independent and dependent variables in a data set using mathematical formular. They can also be used in projecting new relationship between the dependent and independent variables that has not been discovered.
Even though linear models - linear regression - are quite simple to develop and understand, as well as good at predicting linear relationships, their approximation of nonlinear relationships have been found to be mostly unsatisfactory \cite{article_lee}. This paper goes more into the mathematical details 
this paper gives a profound, yet understandable mathematical description of Linear regression, relevant parts of the framework -Sci-Kit- used to perform predictions on the example dataset in the context of Machine learning using the Python programming language.

\section{Theoretical Background}
\subsection{Machine Learning}
According to \cite{ibm_cloud_education_2020}, "Machine learning is a branch of artificial intelligence (AI) focused on building applications that learn from data and improve their accuracy over time without being programmed to do so". Examples of applications built using machine learning trained models are email filters with the ability to distinguish between desired emails and spams, auto-suggestion/auto-correct in many typing applications, self-driving vehicles, amongst others. Machine learning techniques have the advantage of automatically adapting to change detected in trends from data. Likewise, they help reduce the complexity in writing application that are somethings difficulty or even impossible to implement using traditional algorithms. The approach of solving an application complexity using machine learning is shown in figure \ref{fig:machine_approach}.Methods used in training a Machine learning model can be grouped into three main categories.


\begin{figure}[htbp]
	\centerline{\includegraphics [scale=0.32]{figures/machine_approach.png}}
	\caption{Machine Learning approach}
	\label{fig:machine_approach}
\end{figure}


	\begin{itemize}
		\item Supervised Learning
		
		This is a type of machine learning technique in which the algorithm is fed with an input and output dataset of desired solution. The desired output data is otherwise known as lables. These information help train the machine lerning model in the precise prediction of output when given a related output after being trained. Some examples of common algotithms used for supervised learnings are regression analysis - linear regression, etc.-, decision trees, amongst others.\cite{theobald2017machine} The two types of supervised learning techniques used are classification and regression.
		\item Unsupervised Learning
		
		This refers to the machine learning technique in which the algorithm is only fed with input data for the training process. The model learns "unsupervised" by finding out and grouping patterns in inputted datasets. Unlike supervised learning, they require large amount of unlabeled datasets to train -properly find patterns in datasets.
		\item Semisupervised Learning
		
		this is a learning technique that uses both labeled -input and desired output- and unlabled -only input- datasets for training machine lerning models.
	\end{itemize}
The other methods used in training a machine learning model are reinforcement learning and deep learning. Deep learning is a subset of machine learning who's algorithm defines an artificial neural network -ANN- that is designed to emulate the way an human brain learns, and unsupervised or semisupervised learning technique to train \cite{ibm_cloud_education_2020}. On other hand, reinforcement learning is a reward based learning in the sense that the machine leaning is trained by being given a point for reaction to certain events.

\subsection{Linear Regression Model}
Regression 
is a predictive analysis with a long but glorious history from its successful applications to problems in the statistics and economics domain. Regression is a kind of supervised learning technique for determining the best fit line to describe patterns in data: linear regression uses a straight line to describe these patterns.\cite{theobald2017machine}. The best fit line is the line that reduces the summed squared difference between the value of the line of a certain value \textbf{\textit{x}} and its corresponding \textbf{\textit{y}} values \cite{from main reference}. The mathematical expression for linear regression is:
	\begin{equation}
		y = \beta\label{beta}X + \beta\textsubscript{0}
	\end{equation}
	Where y is the dependent variable, X is the independent variable of the equation, $\beta$ is a cofficient which reperesnts the slope of the regression line, $\beta$\textsubscript{0} is a constant value called the \textbf{bias}. Equation \eqref{beta} is the same as the equation of a straight line in linear algebra.
	
	The independent variable X in \eqref{beta} is calculated using the mathematical expression below:
	\begin{equation}
    		%\frac{\sum{(x-x)(y-y)}{6}
		X = \frac{\sum{}(x-\overline{x})(y-\overline{y})}{\sum{}(x-\overline{x})^2}
		%\sum{}\frac{(x-x)}{5}
	\end{equation}	
	Where $\overline{x}$ and $\overline{y}$ are the mean values of all the respective x and y variables.
	
	
Linear regression analysis can be sub-divided into simple and multiple linear regression depending on the number of their independent variables. If the independent variable is one, the regression is said to be a simple linear regression, but if the independent variable is more than one, it is a multiple linear regression. In this paper, we will make analysis using the simple linear regression model.

\subsection{Model Evaluation}
The prediction made by the model using equation \eqref{beta} can be evaluated using the statistical methods below to find out how close the predicted value is to the the actual value.
\subsubsection{R\textsuperscript{2}}
this is otherwise known as the coefficient of determination or the coefficient of multiple determination is a measure of how close the best fit line is to the original data using a simple mean. The value of the output of this calculation ranges from 0 to 1\cite{DAE79009}. A value of 1 refers to the fact that all the points fits 100$\%$ to the the regression line. The lesser the value, the farther the points are to the regression line. The coefficient of determination is defined by the following equation:

	\begin{equation}
    		%\frac{\sum{(x-x)(y-y)}{6}
		R\textsuperscript{2} = \label{three}\frac{\sum_{i=1}^{n}(\hat{y_i}-\overline{y})^2}{\sum_{i=1}^{n}(y_i-\overline{y})^2}
		%\sum{}\frac{(x-x)}{5}
	\end{equation}
	
	Where n is the number of observations i.e. total number of variables, $\hat{y_i}$ is the estimated value of the dependent variable for the i\textsuperscript{th} observation computed by the regression equation, $y_i$ is the observed value of the dependent variable for the i\textsuperscript{th} observation and $\overline{y}$ is the mean of all n observations of the dependent variable \cite{DAE79009}

\subsubsection{Mean Squared Error}
Mean Squared Error (MSE) is a performance measure that helps determine how much error is made in the predicted value in relation to the the actual value of the ouput given the same input. The equation of the calculation is denoted by:

	\begin{equation}
    		%\frac{\sum{(x-x)(y-y)}{6}
		MSE = \label{four}\frac{1}{n}\sum_{i=1}^{n}(y_i-\hat{f}(x_i))^2
		%\sum{}\frac{(x-x)}{5}
	\end{equation}
	where $x_i$ is a vector of all i\textsuperscript{ith} observation, $\hat{f}$ is the prediction function. The lower the value of MSE, the more accurate the prediction is.
\section{Algorithm Implementation}
All the algorithm needed  for prediction using the linear regression model have been implemented in machine learning libraries like Scikit learn which we will be using in this paper.
\subsection{Libraries and Tools used}
\subsubsection{Scikit Learn}
Scikit learn is an open-source python library that provides algorithms that are used in machine learning. This library provide functionalities for solving machine learning jobs like 
regression, classification, clustering, model selection, pre-processing - like spliting data sets into test and train subsets -amongst others \cite{scikit_learn}. The scikit-learn API are designed around this main design principles which are consistency - all objects (basic or composite) share a consistent interface composed of a limited set of methods -, inspection - parameters are exposed as public attributes -, composition, sensible defaults - provides understandable default parameters which gives baseline solution for tasks at hand -and nonproliferation of classes - datasets are represented as NumPy arrays or SciPy sparse matrices \cite{sklearn_api}.
\subsubsection{Numpy}
NumPy is an open-source Python library that that provides routines that allows for fast operations on multidimensional arrays, including mathematical, logical, shape manipulation, sorting, selecting, I/O, discrete Fourier transforms, basic linear algebra, basic statistical operations, random simulation\footnote{https://numpy.org/doc/stable/user/whatisnumpy.html}.
\subsubsection{Pandas}
Pandas is an open-source data analysis and manipulation python library that allows high-performance easy exploration, cleaning and processing of tabular data structures in Python\footnote{https://pandas.pydata.org/docs/}.
\subsubsection{Matplotlib}
Matplotlib is a Python library used along with Numpy for creating static, animated and interactive visualization of data\footnote{https://matplotlib.org/}.
\subsubsection{Seaborn}
Seaborn is a Python library which builds upon the functionalities of matplotlib and integrates closely with pandas data structures for making statistical graphics to datasets\footnote{https://seaborn.pydata.org/introduction.html}.

 Figure \ref{fig:placeholder} shows the import statments for the functions and libraries mentioned above in python.
 \begin{figure}[htbp]
	\centerline{\includegraphics [scale=0.45]{figures/import_statements.png}}
	\caption{Import statements}
	\label{fig:placeholder}
\end{figure}
\section{Practical Example}
In this paper, the california housing datasets from \cite{KELLEYPACE1997291} included in the scikit sklearn datasets library is used to  train and predict the average prices of housing in california using linear regression. 
\subsection{Inspecting Dataset}
The dataset used in this paper is gotten from scikit-learn dataset repository using. 
The dataset has 20,640 instance number, 8 numeric predictive atrributes and 1 target attribute which is expected output of the prediction, which is the median house value for California districts. A block group is the smallest geographical unit for which the U.S. Census Bureau publishes sample data (a block group typically has a population of 600 to 3,000 people), the datasets were obtained in a 1990 census \cite{KELLEYPACE1997291}.
The predictive attributes have the following information: 
	\begin{itemize}
		\item MedInc: represents the median income in a block.
		\item HouseAge: represents the median house age in block.
        		\item AveRooms: represents the average number of rooms.
        		\item AveBedrms: represents the average number of bedrooms.
        		\item Population: represents the population in the block.
        		\item AveOccup: represents the average house occupancy.
	        \item Latitude: represents the house block latitude.
        		\item Longitude: represents the house block longitude.
	\end{itemize}
A summary of these attributes and the corresponding target values as pandas dataframe is depicted in figure \ref{fig:overview_dataset_with_target}. The target attribute represents the median house value in unit of 100,000.

\begin{figure}[htbp]
	\centerline{\includegraphics [scale=0.32]{figures/overview_dataset_with_target.png}}
	\caption{Dataset overview including the target column}
	\label{fig:overview_dataset_with_target}
\end{figure}

The correlation between the attributes is shown in figure \ref{fig:correlation_heatmap}. Figure shows the pearson correlation between the attributes as a heatmap. The darker the blue colour and the closer the value is to 1, the higher the correlation of the attribure to each other. It can be seen from figure \ref{fig:correlation_heatmap} that the median income income in a block has the highest correlation to the target value followed by house age and average rooms. The other values show little or no correlation to the target value because of their negative values.

\begin{figure}[htbp]
	\centerline{\includegraphics [scale=0.5]{figures/heatmap_correlation.png}}
	\caption{Pearson correlation heatmap of the dataset attributes}
	\label{fig:correlation_heatmap}
\end{figure}

\subsection{Split Dataset and Train Model}
The dataset is split into training and testing data using the $train\_test\_split$ function provided by the $sklearn.model\_selection$ class. The dataset was split into 20\% - 4,218 - for testing purposes and 80\% -16, 512 - for training purposes. Split the dataset gives the opportunity of setting aside some dataset for testing purposes to efficiently evaluate the accuracy of the trained model. If the dataset were not split, there is probability of the model predicting the target value through memorising and we want to avoid this. 

An instance of the linear regression model class was instanced, and the training datasets were fed into the $fit$ function which is an implementation of the linear regression formula. The value of the coefficients after training the model is [ 4.48674910e-01  9.72425752e-03 -1.23323343e-01  7.83144907e-01
 -2.02962058e-06 -3.52631849e-03 -4.19792487e-01 -4.33708065e-01]. The value of the intercept was found to be -37.023277706063894. Ideally, these are the values that are needed if manual calculation of the target value $y$ is to be made. 

\subsection{Result and Discussion}
With the regression model successfully trained, the testing datasets can now be fed to the model to predict the target values. The coefficient of determination $R^2$ of training datasets for the model was found to be 0.6125511913966952.
When predictions were made using the testing dataset, there were some offsets in the predicted values to the target value as shown in figure thatttttt which shows a graph of the first 20 values. The Mean squared error of the predicted value to the target value was found to be 0.56 and coefficient of determination to be 0.58. These values explains the reason why there were some offsets in the predictions.

With the result of the predictions from the model, it can be said that the model has underfitted the training data because the features used did not have an accurate linear corellation. This can be seen in figure \ref{fig:correlation_heatmap} which showed some negative values. We can also understand that a lot of not so known factors influence house prices which were not included in the datasets.


\section{Conclusion}
As can be highlighted from the linear regression equation, it is a fairly straightforward regression model to get started with machine learning. However, there are quite poor at predicting with datasets that do not have strong linear correlation as depicted in the trained model in this paper. As a suggestion, different regression algorithms should be used to train datasets to be able to find the best fit for the desired prediction job.

\bibliographystyle{plain}
\bibliography{references}
\end{document}
